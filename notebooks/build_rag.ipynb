{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mypy: disable-error-code=\"import-not-found\"\n",
    "\n",
    "from __future__ import annotations  # noqa: F404\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "from typing import TYPE_CHECKING, List, Tuple\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    import pathlib\n",
    "\n",
    "    from langchain.schema import Document\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import yaml\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# The notebook should be executed from the project root directory\n",
    "if \"_correct_path\" not in locals():\n",
    "    os.chdir(\"..\")\n",
    "    sys.path.append(\".\")\n",
    "    print(f\"changed dir to {Path('.').resolve()})\")\n",
    "    _correct_path = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from infra.settings_rag import diy_rag_nb_output\n",
    "except ImportError:\n",
    "    raise ValueError(\n",
    "        \"Make sure you have set rag_type=RAGType.DIY in `settings_main.py` before using this notebook.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class DiyVectorStoreSettings(BaseModel):\n",
    "    \"\"\"Validation schema for VDB settings.\"\"\"\n",
    "\n",
    "    sentence_transformer_model_name: str\n",
    "    chunk_size: int\n",
    "    chunk_overlap: int\n",
    "\n",
    "\n",
    "PATH_TO_DOCS = \"assets/datarobot_english_documentation_docsassist.zip\"\n",
    "\n",
    "VECTORSTORE_SETTINGS = DiyVectorStoreSettings(\n",
    "    sentence_transformer_model_name=\"all-MiniLM-L6-v2\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "\n",
    "# Settings for caching doc chunking & vectorization\n",
    "cache_dir = \"./.cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "memory = Memory(cache_dir, verbose=0)\n",
    "memory.reduce_size(bytes_limit=512e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk documents and build vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(\n",
    "    path_to_source_documents: pathlib.Path, chunk_size: int, chunk_overlap: int\n",
    ") -> List[Document]:\n",
    "    \"\"\"Convert raw documents into document chunks that can be ingested into a vector db.\"\"\"\n",
    "\n",
    "    def _format_metadata(docs: list[Document]) -> None:\n",
    "        \"\"\"\n",
    "        this function formats doc metadata to extract a valid URL\n",
    "\n",
    "        adapt to the needs of your specific document collection\n",
    "        \"\"\"\n",
    "        https_string = re.compile(r\".+(https://.+)$\")\n",
    "\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = (\n",
    "                doc.metadata[\"source\"]\n",
    "                .replace(\"|\", \"/\")\n",
    "                .replace(str(path_to_source_documents.resolve()), \"\")\n",
    "            )\n",
    "\n",
    "            doc.metadata[\"source\"] = re.sub(\n",
    "                r\"datarobot_docs/en/(.+)\\.txt\",\n",
    "                r\"https://docs.datarobot.com/en/docs/\\1.html\",\n",
    "                doc.metadata[\"source\"],\n",
    "            )\n",
    "            try:\n",
    "                doc.metadata[\"source\"] = https_string.findall(doc.metadata[\"source\"])[0]\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    SOURCE_DOCUMENTS_FILTER = \"**/*.*\"  # \"**/*.pdf\" or \"**/*.txt\"\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        str(path_to_source_documents.resolve()), glob=SOURCE_DOCUMENTS_FILTER\n",
    "    )\n",
    "    splitter = MarkdownTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "    nltk.download(\"punkt_tab\", quiet=True)\n",
    "    nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "\n",
    "    data = loader.load()\n",
    "    docs = splitter.split_documents(data)\n",
    "\n",
    "    _format_metadata(docs)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def process_zip_documents(\n",
    "    path_to_docs_zip: Path, chunk_size: int, chunk_overlap: int\n",
    ") -> List[Document]:\n",
    "    \"\"\"Unzip documents to a temp dir and chunk.\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        with zipfile.ZipFile(path_to_docs_zip, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(temp_dir)\n",
    "        return make_chunks(Path(temp_dir), chunk_size, chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def make_vector_db(\n",
    "    documents: List[Document],\n",
    "    embedding_model_name: str,\n",
    "    embedding_model_output_dir: Path,\n",
    "    vdb_output_dir: Path,\n",
    ") -> Tuple[Path, Path]:\n",
    "    \"\"\"Build the vector db and persist it to disk.\"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        cache_folder=str(embedding_model_output_dir),\n",
    "    )\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    metadatas = [doc.metadata for doc in documents]\n",
    "\n",
    "    db = FAISS.from_texts(texts, embedding_function, metadatas=metadatas)\n",
    "    db.save_local(str(vdb_output_dir))\n",
    "    return embedding_model_output_dir, vdb_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chunking documents...\")\n",
    "doc_chunks = process_zip_documents(\n",
    "    path_to_docs_zip=PATH_TO_DOCS,\n",
    "    chunk_size=VECTORSTORE_SETTINGS.chunk_size,\n",
    "    chunk_overlap=VECTORSTORE_SETTINGS.chunk_overlap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building vector database...\")\n",
    "embedding_path, db_path = make_vector_db(\n",
    "    documents=doc_chunks,\n",
    "    embedding_model_name=VECTORSTORE_SETTINGS.sentence_transformer_model_name,\n",
    "    embedding_model_output_dir=diy_rag_nb_output.embedding_model,\n",
    "    vdb_output_dir=diy_rag_nb_output.vdb,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export settings needed at retrieval time to the RAG deployment directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docsassist.schema import RAGModelSettings\n",
    "\n",
    "rag_model_settings = RAGModelSettings(\n",
    "    embedding_model_name=VECTORSTORE_SETTINGS.sentence_transformer_model_name,\n",
    "    max_retries=0,\n",
    "    request_timeout=30,\n",
    "    temperature=0.0,\n",
    "    stuff_prompt=textwrap.dedent(\"\"\"\\\n",
    "            Use the following pieces of context to answer the user's question.\n",
    "            If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "            ----------------\n",
    "            {context}\"\"\"),\n",
    ")\n",
    "\n",
    "with open(diy_rag_nb_output.rag_settings, \"w\") as f:\n",
    "    yaml.safe_dump(rag_model_settings.model_dump(mode=\"json\"), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
